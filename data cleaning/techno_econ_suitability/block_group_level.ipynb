{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from rasterstats import zonal_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define TIF File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path_mac = \"/Users/jack/Library/CloudStorage/GoogleDrive-limjackailjk@gmail.com/My Drive/Solar PV Lab/NIMBY Project/Jenny's Regression/Data Sources/\"\n",
    "main_path_win = \"G:/My Drive/Solar PV Lab/NIMBY Project/Jenny's Regression/Data Sources/\"\n",
    "\n",
    "GHI= \"GHI-09188ce2.tif\"\n",
    "protected_land = \"Protected_Land-5745a356.tif\"\n",
    "habitat= \"Habitat-32079c87.tif\"\n",
    "slope= \"slope_only-2c1658fa.tif\"\n",
    "popl_dens= \"Popl_Density-714f0a64.tif\"\n",
    "Substation = \"distance_to_substation_only-f02c9129.tif\"\n",
    "land_cover=\"Land_Cover-8a2691e6.tif\"\n",
    "# Define the TIF file paths as a list\n",
    "tif_paths = [GHI, protected_land, habitat, slope, popl_dens, Substation, land_cover]\n",
    "tif_paths_full = [main_path_mac + path for path in tif_paths]\n",
    "tif_paths_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_group_bounding_boxes = pd.read_csv(\"../../data/block_group_clean/bounding_box_full_bg.csv\", dtype={'GEOID': str, 'STATEFP': str, 'COUNTYFP': str, 'TRACTCE': str, 'BLKGRPCE': str})\n",
    "\n",
    "\n",
    "bg_bounding_box_file_path = \"\"\n",
    "bg_bb_full = gpd.read_file(bg_bounding_box_file_path)[['GEOID', 'geometry']] # Not in repo due to size\n",
    "\n",
    "# Merge the bounding boxes with the block group geometries\n",
    "block_group_bounding_boxes = bg_bb_full.merge(block_group_bounding_boxes, on='GEOID', how='left')\n",
    "\n",
    "block_group_bounding_boxes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import process_tif_files\n",
    "col_names = [\"GHI\", \"Protected_Land\", \"Habitat\", \"Slope\", \"Population_Density\", \"Distance_to_Substation\", \"Land_Cover\"]\n",
    "\n",
    "block_group_bounding_boxes_4326 = block_group_bounding_boxes.to_crs(\"EPSG:4326\")\n",
    "block_group_suitability_scores = process_tif_files(tif_paths_full, block_group_bounding_boxes_4326[['geometry', 'GEOID', 'County Name', 'State', 'TRACTCE', \"BLKGRPCE\"]], nodata_value=np.nan, bg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix weird errors with Conneticut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = {\n",
    "    '110': 'Hartford', '190': 'Fairfield', '170': 'Litchfield', \n",
    "    '140': 'Middlefield', '120': 'New Haven', '130': 'Tolland',\n",
    "    '160': 'Windham', '180': 'New London', '150': 'New London'\n",
    "}\n",
    "\n",
    "def fix_data_Connecticut(series):\n",
    "    \n",
    "    if series['GEOID'][:2] == '09':\n",
    "        series['State'] = 'Connecticut'\n",
    "        series['County Name'] = mapper[series['GEOID'][2:5]]\n",
    "        series['TRACTCE'] = series['GEOID'][5:11]\n",
    "        series['BLKGRPCE'] = series['GEOID'][11:]\n",
    "\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_group_suitability_scores = block_group_suitability_scores.apply(fix_data_Connecticut, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix certain states and tracts that are not needed/weird code matching errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIPS = pd.read_csv(\"../../data/extras/US_FIPS_Codes.csv\", dtype={'FIPS State': str, 'FIPS County': str})\n",
    "\n",
    "state_dict = FIPS.set_index('FIPS State')['State'].to_dict()\n",
    "\n",
    "# County dict for mapping but requires to match the FIPS State as well\n",
    "county_dict = FIPS.set_index(['FIPS State', 'FIPS County'])['County Name'].to_dict()\n",
    "def fix_tract(series):\n",
    "    if series['GEOID'] != np.nan:\n",
    "        if series['GEOID'][:2] == '02':\n",
    "            series['State'] = 'Alaska'\n",
    "        elif series['GEOID'][:2] == '15':\n",
    "            series['State'] = 'Hawaii'\n",
    "        else:\n",
    "            try:\n",
    "                series['State'] = state_dict[series['GEOID'][:2]]\n",
    "            except:\n",
    "                series['State'] = np.nan\n",
    "        series['TRACTCE'] = series['GEOID'][5:11]\n",
    "        series['BLKGRPCE'] = series['GEOID'][11:]\n",
    "        try:\n",
    "            series['County Name'] = county_dict[(series['GEOID'][:2], series['GEOID'][2:5])]\n",
    "        except:\n",
    "            series['County Name'] = np.nan\n",
    "        \n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_group_suitability_scores = block_group_suitability_scores.apply(fix_tract, axis=1)\n",
    "block_group_suitability_scores = block_group_suitability_scores.dropna(subset=['State', 'County Name'])\n",
    "block_group_suitability_scores.to_csv('block_group_suitability_scores.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PowerLab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
